{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ§  Encephlo â€” DenseNet121 Training Pipeline\n",
    "**Brain Tumor MRI Classification | Grad-CAM Compatible | Target: â‰¥96% Accuracy**\n",
    "\n",
    "Classes: `glioma` | `meningioma` | `notumor` | `pituitary`\n",
    "\n",
    "Architecture strategy:\n",
    "- **Phase 1** â€” Freeze DenseNet121 base, train new top layers (warm-up)\n",
    "- **Phase 2** â€” Unfreeze last 50 layers, fine-tune at low LR\n",
    "\n",
    "Grad-CAM target layer: `conv5_block16_concat` (last concat in Dense Block 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ğŸ“¦ Imports & Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.applications.densenet import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    ")\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "# â”€â”€ Reproducibility â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "print(f'TensorFlow version : {tf.__version__}')\n",
    "print(f'GPU available      : {tf.config.list_physical_devices(\"GPU\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. âš™ï¸ Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "BASE_DIR     = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "TRAIN_DIR    = os.path.join(BASE_DIR, '..', 'MRI images', 'Training')\n",
    "TEST_DIR     = os.path.join(BASE_DIR, '..', 'MRI images', 'Testing')\n",
    "MODELS_DIR   = os.path.join(BASE_DIR, 'models')\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "MODEL_SAVE_PATH = os.path.join(MODELS_DIR, 'densenet121.keras')\n",
    "BEST_WEIGHTS    = os.path.join(MODELS_DIR, 'densenet121_best.weights.h5')\n",
    "\n",
    "# â”€â”€ Hyperparameters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "IMG_SIZE     = (224, 224)\n",
    "IMG_SHAPE    = (224, 224, 3)\n",
    "BATCH_SIZE   = 32\n",
    "VAL_SPLIT    = 0.15            # 15% of training set for validation\n",
    "CLASS_NAMES  = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
    "NUM_CLASSES  = len(CLASS_NAMES)\n",
    "\n",
    "# Phase 1 â€” warm-up (frozen base)\n",
    "P1_EPOCHS    = 15\n",
    "P1_LR        = 1e-3\n",
    "\n",
    "# Phase 2 â€” fine-tune\n",
    "P2_EPOCHS    = 35\n",
    "P2_LR        = 1e-5\n",
    "UNFREEZE_FROM = -50           # unfreeze last 50 layers for fine-tuning\n",
    "\n",
    "# Grad-CAM target layer name (last conv concat in DenseNet121 Dense Block 4)\n",
    "GRADCAM_LAYER = 'conv5_block16_concat'\n",
    "\n",
    "print(f'Train dir  : {TRAIN_DIR}')\n",
    "print(f'Test  dir  : {TEST_DIR}')\n",
    "print(f'Models dir : {MODELS_DIR}')\n",
    "\n",
    "# Quick sanity: count images per class\n",
    "print('\\nâ”€â”€ Training class distribution â”€â”€')\n",
    "for cls in CLASS_NAMES:\n",
    "    path = os.path.join(TRAIN_DIR, cls)\n",
    "    count = len(os.listdir(path)) if os.path.exists(path) else 0\n",
    "    print(f'  {cls:<15}: {count} images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ğŸ“‚ Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Augmentation for training â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function = preprocess_input,   # DenseNet121-specific normalisation\n",
    "    validation_split       = VAL_SPLIT,\n",
    "    rotation_range         = 20,\n",
    "    width_shift_range      = 0.15,\n",
    "    height_shift_range     = 0.15,\n",
    "    shear_range            = 0.1,\n",
    "    zoom_range             = 0.2,\n",
    "    horizontal_flip        = True,\n",
    "    brightness_range       = [0.8, 1.2],\n",
    "    fill_mode              = 'nearest'\n",
    ")\n",
    "\n",
    "# â”€â”€ No augmentation for test/val â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "val_test_datagen = ImageDataGenerator(\n",
    "    preprocessing_function = preprocess_input\n",
    ")\n",
    "\n",
    "# â”€â”€ Generators â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "train_gen = train_datagen.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size   = IMG_SIZE,\n",
    "    batch_size    = BATCH_SIZE,\n",
    "    class_mode    = 'categorical',\n",
    "    classes       = CLASS_NAMES,\n",
    "    subset        = 'training',\n",
    "    seed          = SEED,\n",
    "    shuffle       = True\n",
    ")\n",
    "\n",
    "val_gen = train_datagen.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size   = IMG_SIZE,\n",
    "    batch_size    = BATCH_SIZE,\n",
    "    class_mode    = 'categorical',\n",
    "    classes       = CLASS_NAMES,\n",
    "    subset        = 'validation',\n",
    "    seed          = SEED,\n",
    "    shuffle       = False\n",
    ")\n",
    "\n",
    "test_gen = val_test_datagen.flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size   = IMG_SIZE,\n",
    "    batch_size    = BATCH_SIZE,\n",
    "    class_mode    = 'categorical',\n",
    "    classes       = CLASS_NAMES,\n",
    "    shuffle       = False\n",
    ")\n",
    "\n",
    "print(f'\\nTrain batches : {len(train_gen)}')\n",
    "print(f'Val   batches : {len(val_gen)}')\n",
    "print(f'Test  batches : {len(test_gen)}')\n",
    "print(f'Class indices : {train_gen.class_indices}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Visualise a batch sample â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig, axes = plt.subplots(2, 4, figsize=(14, 7))\n",
    "batch_imgs, batch_labels = next(train_gen)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    # Denormalise DenseNet preprocess for display\n",
    "    img_display = batch_imgs[i].copy()\n",
    "    img_display = (img_display - img_display.min()) / (img_display.max() - img_display.min())\n",
    "    ax.imshow(img_display)\n",
    "    label_idx = np.argmax(batch_labels[i])\n",
    "    ax.set_title(CLASS_NAMES[label_idx], fontsize=10)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Sample Augmented Training Images', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ğŸ—ï¸ Model Architecture (Grad-CAM Compatible Flat Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_densenet121_model(num_classes: int, input_shape: tuple) -> Model:\n",
    "    \"\"\"\n",
    "    Builds a FLAT (non-nested) DenseNet121 classifier.\n",
    "    \n",
    "    CRITICAL for Grad-CAM: We DO NOT wrap the base in a separate Model().\n",
    "    Instead, we call base_model.layers directly into a functional API chain.\n",
    "    This ensures make_gradcam_heatmap() in utils.py works with the\n",
    "    'STANDARD FLAT MODEL' branch (line 41 of utils.py).\n",
    "    \n",
    "    Grad-CAM target layer: 'conv5_block16_concat'\n",
    "    This is the output of the last dense block in DenseNet121 â€” rich\n",
    "    spatial features, perfect for visualising tumour localisation.\n",
    "    \"\"\"\n",
    "    # Load pre-trained base (ImageNet weights)\n",
    "    base = DenseNet121(\n",
    "        include_top  = False,\n",
    "        weights      = 'imagenet',\n",
    "        input_shape  = input_shape\n",
    "    )\n",
    "    base.trainable = False   # Freeze for warm-up phase\n",
    "    \n",
    "    # â”€â”€ Functional API â€” FLAT chain (Grad-CAM requirement) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    inputs  = keras.Input(shape=input_shape, name='input_layer')\n",
    "    x       = base(inputs, training=False)          # Base outputs feature maps\n",
    "    \n",
    "    # NOTE: because base is called as a callable (not wrapped in Model()),\n",
    "    # the resulting graph is FLAT â€” utils.py's standard branch handles this.\n",
    "    # However, since base IS a sub-model object, utils.py will detect it via\n",
    "    # isinstance(layer, tf.keras.Model). We handle this by INSTEAD using\n",
    "    # the Functional API properly so the inner_model branch works correctly.\n",
    "    \n",
    "    x       = layers.GlobalAveragePooling2D(name='gap')(x)\n",
    "    x       = layers.BatchNormalization(name='bn_top')(x)\n",
    "    x       = layers.Dense(512, activation='relu', name='fc_512')(\n",
    "                  x, )\n",
    "    x       = layers.Dropout(0.4, name='dropout_1')(x)\n",
    "    x       = layers.Dense(256, activation='relu', name='fc_256')(x)\n",
    "    x       = layers.Dropout(0.3, name='dropout_2')(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax', name='predictions')(x)\n",
    "    \n",
    "    model   = Model(inputs=inputs, outputs=outputs, name='Encephlo_DenseNet121')\n",
    "    return model, base\n",
    "\n",
    "\n",
    "model, base_model = build_densenet121_model(NUM_CLASSES, IMG_SHAPE)\n",
    "model.summary()\n",
    "\n",
    "print(f'\\nTotal layers in base  : {len(base_model.layers)}')\n",
    "print(f'Trainable params (P1) : {sum([p.numpy().size for p in model.trainable_weights]):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Verify Grad-CAM target layer exists â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "try:\n",
    "    target_layer = base_model.get_layer(GRADCAM_LAYER)\n",
    "    print(f'âœ… Grad-CAM layer found: \"{GRADCAM_LAYER}\"')\n",
    "    print(f'   Output shape: {target_layer.output_shape}')\n",
    "except ValueError as e:\n",
    "    print(f'âŒ Layer not found: {e}')\n",
    "    print('Available last-block layers:')\n",
    "    for l in base_model.layers:\n",
    "        if 'block16' in l.name or 'block15' in l.name:\n",
    "            print(f'  {l.name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. âš¡ Phase 1 â€” Warm-Up Training (Frozen Base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Callbacks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "callbacks_p1 = [\n",
    "    EarlyStopping(\n",
    "        monitor              = 'val_accuracy',\n",
    "        patience             = 5,\n",
    "        restore_best_weights = True,\n",
    "        verbose              = 1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor  = 'val_loss',\n",
    "        factor   = 0.5,\n",
    "        patience = 3,\n",
    "        min_lr   = 1e-7,\n",
    "        verbose  = 1\n",
    "    )\n",
    "]\n",
    "\n",
    "# â”€â”€ Compile â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "model.compile(\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=P1_LR),\n",
    "    loss      = 'categorical_crossentropy',\n",
    "    metrics   = ['accuracy']\n",
    ")\n",
    "\n",
    "print('â”€â”€ Phase 1: Warm-up (base frozen) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€')\n",
    "print(f'Learning rate : {P1_LR}')\n",
    "print(f'Max epochs    : {P1_EPOCHS}')\n",
    "print(f'Trainable params: {model.trainable_variables[-1].shape}')\n",
    "print('Launching training...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_p1 = model.fit(\n",
    "    train_gen,\n",
    "    validation_data  = val_gen,\n",
    "    epochs           = P1_EPOCHS,\n",
    "    callbacks        = callbacks_p1,\n",
    "    verbose          = 1\n",
    ")\n",
    "\n",
    "p1_best_val_acc = max(history_p1.history['val_accuracy'])\n",
    "print(f'\\nâ”€â”€ Phase 1 Complete â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€')\n",
    "print(f'Best val accuracy: {p1_best_val_acc:.4f} ({p1_best_val_acc*100:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Phase 1 Learning Curves â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(history_p1.history['accuracy'],     label='Train Acc',  color='#2196F3')\n",
    "ax1.plot(history_p1.history['val_accuracy'], label='Val Acc',    color='#4CAF50')\n",
    "ax1.set_title('Phase 1 â€” Accuracy', fontweight='bold')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "ax2.plot(history_p1.history['loss'],     label='Train Loss', color='#F44336')\n",
    "ax2.plot(history_p1.history['val_loss'], label='Val Loss',   color='#FF9800')\n",
    "ax2.set_title('Phase 1 â€” Loss', fontweight='bold')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('DenseNet121 â€” Phase 1 Training (Frozen Base)', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ğŸ”“ Phase 2 â€” Fine-Tuning (Unfreeze Last 50 Layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Unfreeze last N layers of the base model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "base_model.trainable = True\n",
    "\n",
    "# Freeze all layers EXCEPT the last UNFREEZE_FROM (-50 means last 50)\n",
    "for layer in base_model.layers[:UNFREEZE_FROM]:\n",
    "    layer.trainable = False\n",
    "\n",
    "trainable_count = sum(1 for l in base_model.layers if l.trainable)\n",
    "print(f'Trainable base layers : {trainable_count} / {len(base_model.layers)}')\n",
    "\n",
    "# â”€â”€ Callbacks for Phase 2 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "callbacks_p2 = [\n",
    "    EarlyStopping(\n",
    "        monitor              = 'val_accuracy',\n",
    "        patience             = 10,\n",
    "        restore_best_weights = True,\n",
    "        verbose              = 1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor  = 'val_loss',\n",
    "        factor   = 0.3,\n",
    "        patience = 4,\n",
    "        min_lr   = 1e-8,\n",
    "        verbose  = 1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        filepath             = BEST_WEIGHTS,\n",
    "        monitor              = 'val_accuracy',\n",
    "        save_best_only       = True,\n",
    "        save_weights_only    = True,\n",
    "        verbose              = 1\n",
    "    )\n",
    "]\n",
    "\n",
    "# â”€â”€ Recompile at VERY low LR â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "model.compile(\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=P2_LR),\n",
    "    loss      = 'categorical_crossentropy',\n",
    "    metrics   = ['accuracy']\n",
    ")\n",
    "\n",
    "print(f'\\nâ”€â”€ Phase 2: Fine-Tuning â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€')\n",
    "print(f'Learning rate : {P2_LR}')\n",
    "print(f'Max epochs    : {P2_EPOCHS}')\n",
    "print('Launching fine-tuning...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_p2 = model.fit(\n",
    "    train_gen,\n",
    "    validation_data  = val_gen,\n",
    "    epochs           = P2_EPOCHS,\n",
    "    callbacks        = callbacks_p2,\n",
    "    verbose          = 1\n",
    ")\n",
    "\n",
    "p2_best_val_acc = max(history_p2.history['val_accuracy'])\n",
    "print(f'\\nâ”€â”€ Phase 2 Complete â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€')\n",
    "print(f'Best val accuracy: {p2_best_val_acc:.4f} ({p2_best_val_acc*100:.2f}%)')\n",
    "\n",
    "# Best weights are already restored by ModelCheckpoint + EarlyStopping\n",
    "model.load_weights(BEST_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Combined Training History â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def combine_histories(h1, h2):\n",
    "    combined = {}\n",
    "    for key in h1.history:\n",
    "        combined[key] = h1.history[key] + h2.history[key]\n",
    "    return combined\n",
    "\n",
    "full_history = combine_histories(history_p1, history_p2)\n",
    "p1_end = len(history_p1.history['accuracy'])\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "for ax, metric, ylabel in [\n",
    "    (ax1, 'accuracy', 'Accuracy'),\n",
    "    (ax2, 'loss',     'Loss')\n",
    "]:\n",
    "    ax.plot(full_history[metric],         label=f'Train {ylabel}', color='#2196F3')\n",
    "    ax.plot(full_history[f'val_{metric}'],label=f'Val {ylabel}',   color='#4CAF50')\n",
    "    ax.axvline(x=p1_end - 0.5, color='gray', linestyle='--', alpha=0.7, label='Phase 1 â†’ Phase 2')\n",
    "    ax.set_title(f'Full Training â€” {ylabel}', fontweight='bold')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('DenseNet121 â€” Full Training History', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ğŸ“Š Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Test Accuracy & Loss â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print('Evaluating on unseen test set...')\n",
    "test_loss, test_acc = model.evaluate(test_gen, verbose=1)\n",
    "\n",
    "print(f'\\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•')\n",
    "print(f'  TEST ACCURACY  : {test_acc*100:.2f}%')\n",
    "print(f'  TEST LOSS      : {test_loss:.4f}')\n",
    "print(f'â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•')\n",
    "\n",
    "if test_acc >= 0.96:\n",
    "    print(f'  âœ… TARGET MET â€” {test_acc*100:.2f}% â‰¥ 96.00%')\n",
    "else:\n",
    "    print(f'  âš ï¸  Below target â€” {test_acc*100:.2f}% < 96.00%')\n",
    "    print('     Consider running more fine-tune epochs or adjusting LR.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Predictions for Classification Report + Confusion Matrix â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "test_gen.reset()\n",
    "y_pred_probs = model.predict(test_gen, verbose=1)\n",
    "y_pred       = np.argmax(y_pred_probs, axis=1)\n",
    "y_true       = test_gen.classes\n",
    "\n",
    "print('\\nâ”€â”€ Classification Report â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€')\n",
    "print(classification_report(y_true, y_pred, target_names=CLASS_NAMES, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Confusion Matrix â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 7))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=CLASS_NAMES)\n",
    "disp.plot(\n",
    "    ax          = ax,\n",
    "    cmap        = 'Blues',\n",
    "    colorbar    = False,\n",
    "    values_format = 'd'\n",
    ")\n",
    "ax.set_title('DenseNet121 â€” Confusion Matrix (Test Set)', fontsize=13, fontweight='bold')\n",
    "plt.xticks(rotation=30, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Per-class accuracy\n",
    "print('\\nâ”€â”€ Per-Class Accuracy â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€')\n",
    "for i, cls in enumerate(CLASS_NAMES):\n",
    "    cls_acc = cm[i, i] / cm[i].sum()\n",
    "    print(f'  {cls:<15}: {cls_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ğŸ’¾ Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(MODEL_SAVE_PATH)\n",
    "print(f'\\nâœ… Model saved to: {MODEL_SAVE_PATH}')\n",
    "print(f'   File size: {os.path.getsize(MODEL_SAVE_PATH) / 1e6:.1f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ğŸ”¥ Grad-CAM Smoke Test\n",
    "Verifies the model is fully compatible with `utils.py`'s `make_gradcam_heatmap()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Inline Grad-CAM (mirrors utils.py logic exactly) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    \"\"\"\n",
    "    Exact copy of Encephlo's utils.py â€” validates compatibility.\n",
    "    The model has a nested base (DenseNet121), so the inner_model branch runs.\n",
    "    \"\"\"\n",
    "    inner_model = next(\n",
    "        (layer for layer in model.layers if isinstance(layer, tf.keras.Model)), None\n",
    "    )\n",
    "\n",
    "    if inner_model is not None:\n",
    "        last_conv_layer = inner_model.get_layer(last_conv_layer_name)\n",
    "        grad_base_model = tf.keras.Model(\n",
    "            inner_model.inputs,\n",
    "            [last_conv_layer.output, inner_model.output]\n",
    "        )\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            conv_outputs, base_outputs = grad_base_model(img_array)\n",
    "            tape.watch(conv_outputs)\n",
    "            x = base_outputs\n",
    "            inner_idx = model.layers.index(inner_model)\n",
    "            for layer in model.layers[inner_idx + 1:]:\n",
    "                x = layer(x)\n",
    "            preds = x\n",
    "            if pred_index is None:\n",
    "                pred_index = tf.argmax(preds[0])\n",
    "            class_channel = preds[:, pred_index]\n",
    "\n",
    "        grads = tape.gradient(class_channel, conv_outputs)\n",
    "        pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "        last_conv_layer_output = conv_outputs[0]\n",
    "        heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "        heatmap = tf.squeeze(heatmap)\n",
    "        heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "        return heatmap.numpy()\n",
    "    else:\n",
    "        grad_model = tf.keras.Model(\n",
    "            inputs=model.inputs,\n",
    "            outputs=[model.get_layer(last_conv_layer_name).output, model.output]\n",
    "        )\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = grad_model(img_array)\n",
    "            last_conv_layer_output, preds = outputs\n",
    "            if pred_index is None:\n",
    "                pred_index = tf.argmax(preds[0])\n",
    "            class_channel = preds[:, pred_index]\n",
    "        grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "        pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "        last_conv_layer_output = last_conv_layer_output[0]\n",
    "        heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "        heatmap = tf.squeeze(heatmap)\n",
    "        heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "        return heatmap.numpy()\n",
    "\n",
    "\n",
    "def overlay_heatmap(img, heatmap, alpha=0.4):\n",
    "    h, w = img.shape[:2] if hasattr(img, 'shape') else (img.size[1], img.size[0])\n",
    "    heatmap_uint8 = np.uint8(255 * heatmap)\n",
    "    heatmap_resized = cv2.resize(heatmap_uint8, (w, h))\n",
    "    heatmap_bgr = cv2.applyColorMap(heatmap_resized, cv2.COLORMAP_JET)\n",
    "    heatmap_rgb = cv2.cvtColor(heatmap_bgr, cv2.COLOR_BGR2RGB)\n",
    "    img_array = np.array(img)\n",
    "    superimposed = heatmap_rgb * alpha + img_array\n",
    "    return np.clip(superimposed, 0, 255).astype('uint8')\n",
    "\n",
    "\n",
    "print('Grad-CAM functions loaded âœ…')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Pick one sample from each class and run Grad-CAM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "sample_errors = []\n",
    "\n",
    "for col, cls in enumerate(CLASS_NAMES):\n",
    "    cls_dir = os.path.join(TEST_DIR, cls)\n",
    "    sample_file = os.path.join(cls_dir, os.listdir(cls_dir)[0])\n",
    "    \n",
    "    raw_img = Image.open(sample_file).convert('RGB').resize(IMG_SIZE)\n",
    "    img_arr = np.array(raw_img)\n",
    "    img_prep = preprocess_input(np.expand_dims(img_arr.copy(), axis=0))\n",
    "    \n",
    "    try:\n",
    "        # â”€â”€ Run prediction â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        preds     = model.predict(img_prep, verbose=0)\n",
    "        pred_cls  = CLASS_NAMES[np.argmax(preds)]\n",
    "        pred_conf = preds[0][np.argmax(preds)] * 100\n",
    "        \n",
    "        # â”€â”€ Generate Grad-CAM heatmap â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        heatmap   = make_gradcam_heatmap(img_prep, model, GRADCAM_LAYER)\n",
    "        overlay   = overlay_heatmap(raw_img, heatmap)\n",
    "        \n",
    "        # â”€â”€ Display â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        axes[0][col].imshow(raw_img)\n",
    "        axes[0][col].set_title(f'Original\\n[{cls}]', fontsize=9)\n",
    "        axes[0][col].axis('off')\n",
    "        \n",
    "        axes[1][col].imshow(overlay)\n",
    "        ok = 'âœ…' if pred_cls == cls else 'âŒ'\n",
    "        axes[1][col].set_title(\n",
    "            f'Grad-CAM {ok}\\nPred: {pred_cls}\\n{pred_conf:.1f}%', fontsize=8\n",
    "        )\n",
    "        axes[1][col].axis('off')\n",
    "        \n",
    "    except Exception as e:\n",
    "        sample_errors.append(f'{cls}: {e}')\n",
    "        axes[0][col].text(0.5, 0.5, f'Error\\n{e}', ha='center', va='center', transform=axes[0][col].transAxes)\n",
    "        axes[1][col].axis('off')\n",
    "\n",
    "plt.suptitle(\n",
    "    f'DenseNet121 â€” Grad-CAM Smoke Test\\nLayer: \"{GRADCAM_LAYER}\"',\n",
    "    fontsize=13, fontweight='bold'\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "if sample_errors:\n",
    "    print('âŒ Errors encountered:')\n",
    "    for err in sample_errors:\n",
    "        print(f'  {err}')\n",
    "else:\n",
    "    print('âœ… Grad-CAM smoke test PASSED â€” fully compatible with utils.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. âœ… Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('â•' * 55)\n",
    "print('  ENCEPHLO â€” DenseNet121 Training Summary')\n",
    "print('â•' * 55)\n",
    "print(f'  Model         : DenseNet121 (ImageNet â†’ Fine-tuned)')\n",
    "print(f'  Classes       : {CLASS_NAMES}')\n",
    "print(f'  Input size    : {IMG_SIZE}')\n",
    "print(f'  Grad-CAM layer: {GRADCAM_LAYER}')\n",
    "print(f'  Test Accuracy : {test_acc*100:.2f}%  (target: 96%+)')\n",
    "print(f'  Test Loss     : {test_loss:.4f}')\n",
    "print(f'  Model saved   : {MODEL_SAVE_PATH}')\n",
    "print('â•' * 55)\n",
    "print()\n",
    "print('ENSEMBLE_CONFIG entry for app.py:')\n",
    "print('  \"DenseNet121\": {')\n",
    "print(f'      \"path\" : \"models/densenet121.keras\",')\n",
    "print(f'      \"layer\": \"{GRADCAM_LAYER}\",')\n",
    "print( '      \"preprocess\": tf.keras.applications.densenet.preprocess_input')\n",
    "print('  }')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Encephlo Venv",
   "language": "python",
   "name": "encephlo_venv"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
